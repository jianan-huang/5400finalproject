{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from yahoo_fin import stock_info as si\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news(start_date, end_date, output_csv_path):\n",
    "    companies = [\n",
    "        \"JPM\",  # JPMorgan Chase\n",
    "        # \"GS\",   # Goldman Sachs\n",
    "        # \"PFE\",  # Pfizer\n",
    "        # \"MRNA\", # Moderna\n",
    "        # \"AAPL\", # Apple\n",
    "        # \"MSFT\", # Microsoft\n",
    "        # \"TSLA\", # Tesla\n",
    "        # \"NVDA\"  # Nvidia\n",
    "    ]\n",
    "\n",
    "    all_articles = []\n",
    "\n",
    "    for company in companies:\n",
    "        print(f\"Scraping news for: {company}\")\n",
    "        company_news = scrape_yahoo_finance(company, start_date, end_date)\n",
    "        all_articles.extend(company_news)\n",
    "\n",
    "    # Save the data to a CSV\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"News data saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo_finance(company, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Scrapes Yahoo Finance news for a specific company within a date range.\n",
    "    Supports pagination to fetch multiple pages of news articles.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://finance.yahoo.com/quote/{company}/news?p={company}&count=100\"\n",
    "    page = 1\n",
    "    all_articles = []\n",
    "\n",
    "    while True:\n",
    "        # Construct URL with pagination\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch news for {company}: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        articles = soup.find_all(\"li\", class_=\"js-stream-content\")\n",
    "        \n",
    "        if not articles:  # If no articles are found, exit pagination\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                title_element = article.find(\"h3\")\n",
    "                title = title_element.text if title_element else \"N/A\"\n",
    "                link = \"https://finance.yahoo.com\" + title_element.find(\"a\")[\"href\"] if title_element else \"N/A\"\n",
    "\n",
    "                # Fetch the article content\n",
    "                content = scrape_full_content(link)\n",
    "\n",
    "                all_articles.append({\n",
    "                    \"company\": company,\n",
    "                    \"title\": title,\n",
    "                    \"url\": link,\n",
    "                    \"content\": content,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "                continue\n",
    "\n",
    "        page += 1  # Move to the next page\n",
    "        time.sleep(1)  # Delay to avoid overwhelming the server\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "def scrape_full_content(url):\n",
    "    \"\"\"\n",
    "    Scrapes the full content of a news article given its URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        full_text = \" \".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape article content from {url}: {e}\")\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping news for: JPM\n",
      "Failed to fetch news for JPM: 404\n",
      "News data saved to: test_data/financial_news.csv\n"
     ]
    }
   ],
   "source": [
    "start_date = \"2024-09-27\"\n",
    "end_date = \"2024-11-01\"\n",
    "output_csv_path = \"test_data/financial_news.csv\"\n",
    "\n",
    "fetch_news(start_date, end_date, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5400",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
